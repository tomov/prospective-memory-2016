2. The Model

2.1. General PM Task Design

PM tasks can be categorized as either event-based or time-based. Time-based PM tasks involve a PM intention that is associated with a particular time and usually instruct the subject to perform the PM action after a certain amount of time following the PM instruction. In our study, we will focus exclusively on event-based PM tasks in which the participant is asked to perform the PM action when a particular event occurs, most often when a certain target item appears on the screen (although our model could be extended to support time-based PM; see General Discussion). The typical paradigm for studying event-based PM in an experimental setting proceeds as follows (adapted from ~\cite{EM2005}).

Present participants with instructions and practice trials for an ongoing task (e.g. "pleasantness rating").
Present participants with the PM instructions (e.g. "Press a designated key whenever you see the word 'rake' in the context of the ongoing task").
A delay is introduced during which participants perform other activities (e.g. do other memory tasks, and/or fill out demographics forms)
Reintroduce the ongoing task (pleasantness rating) without reminding participants of the PM task
The PM target ("rake") occurs several times in the ongoing task, and PM performance is measured by the proportion of times participants remember to press the designated key when the target occurs.

In our simulations, we focus on phases 2, 4, and 5: simulating the PM instruction and performing the ongoing task with interspersed PM target stimuli. Phase 1 -- the ongoing task instructions -- is assumed in the structure of the model. That is, we manually construct the model for each experiment such that it performs the ongoing task correctly. This assumes that the stimulus-response mechanisms for correctly performing the ongoing task are already part of the semantic associations of the subject. This is consistent with the typical PM experiment in which the ongoing task is trivial and only taps into prior knowledge without requiring learning of new associations (e.g. semantic categorization ~\cite{EM2005}, letter case identification ~\cite{Gilbert2013}, number parity ~\cite{Momennejad2012}). Similarly, the stimulus-response associations relevant for performing the PM task are also built into the model -- indeed, in most PM experiments, the PM task by itself is trivial (e.g. pressing a button) and requires minimal processing of information. Theoretically, this step could be included as a learning phase ~\cite{Cohen1990} that adjusts the appropriate stimulus-response pathways in the model in order to allow it to perform each of the two tasks independently. While such an approach would more accurately mimic the combination of prior semantic associations plus the several practice trials that enable the subject to perform the tasks successfully, it is not the focus of the present article and could potentially be addressed in a future study (see General Discussion).

The first step in a simulation of a PM experiment is therefore presenting the PM instruction to the model. This includes encoding the PM target-task association in the long-term memory store of the model via a single-trial learning process. It also includes setting the activations in the temporary memory store to appropriate levels that allow the model to perform the ongoing and the PM tasks according to the task instructions. We assume that this occurs in a short preparatory phase during which the subject puts herself in a state of mind that would allow her to successfully perform the task in phases 4 and 5. We also skip phase 3 -- the distraction period before commencing the actual experiment -- because any effects of the distraction can be accounted for by adjusting the activation levels in the temporary memory store. Finally, the main part of the experiment begins as a sequence of ongoing stimuli is presented in which the model subject responds to each stimulus according to the ongoing task instructions. A few of these stimuli are also targets and the appropriate PM response must be performed according to the PM task instructions.

In this article, we adopt a similar terminology as the one used by Gilbert et al. (2013) ~\cite{Gilbert2013}. We refer to a single presentation of a stimulus followed by a single response as a trial. Each trial is either an ongoing trial in which the stimulus is not a target, or a PM trial in which the stimulus is a PM target. A correct PM response on a PM trial is referred to as aPM hit. Conversely, a PM miss occurs when an ongoing response is performed on a PM trial (i.e. the subject failed to notice the PM target and switch to the PM task). Similarly, a false alarm occurs when a PM response is made on an ongoing trial (i.e. the subject mistakenly took the ongoing stimulus for a target). Finally, a sequence of trials (ongoing trials and/or PM trials) under the same PM instruction is referred to as a block of trials. Therefore a full simulation of an experiment with our model begins with a PM instruction followed by one or more blocks of trials, in which each block contains ongoing trials and some blocks also contain PM trials.

We assess the validity of our model by measuring the response times (RT’s) and PM hit rates in different PM conditions and comparing them to the RT’s and PM hit rates reported in the literature for the same PM conditions. There are various aspects of the ongoing task and the PM task that influence RT’s and accuracies on both the PM and the ongoing task, such as their level of difficulty, the amount of emphasis on the PM task, the focality of the PM target, the number of PM targets, etc. We discuss each of those factors and how our model accounts for them individually as they appear in the different simulations.

2.2. The PDP Framework

The model was developed within the general PDP framework described by Rumelhart, Hinton, and McClelland (1986) ~\cite{Rumelhart1986}. It consists of processing units (“nodes”) grouped into modules (or “layers”) that communicate with each other via weighted connections. Each unit is an information processing device that has a certain real-valued activation level. The pattern of activation of the units in a given layer represents information that the layer encodes. Information flows across the network through connections between the individual units, governed by the weights of these connections. Units in each layer receive input through connections from units in other layers or in the same layer. These input signals modulate the activation of the receiving units which in turn send this activation through their connections to the rest of the network. This propagation of activation allows the network to receive input in the form of an activation pattern in one of its layers, perform a computation with that input and elicit a response through a pattern of activation in another layer. 

Connections can exist between units of different layers as well as units from the same layer. In our model, most connections between different layers tend to be excitatory, i.e. increased activation of the units in the sending layer leads to increased activation of the units in the receiving layer. Excitatory inputs are represented by the positive sign of the weight of the connections. If the sign of a connection is negative, it sends inhibitory input to the units on the receiving end and the relationship between their activation and the activation of the sending units is reversed. Most inhibitory connections in our model exist between units of the same layer which implements a form of lateral inhibition whereby increased activation in one unit suppresses the activation of the others, and vice versa. This imposes a limit on the total amount of activation of the units in a given layer and also implements competition which allows the network to converge to a single response. Inhibitory connections were also used between the units in the temporary store of the model in order to allow them to sustain continuous ranges of activations over time while also limiting the amount of information that they can store at any given time. Our model of activation-based memory thus very closely resembles existing models of working memory that rely on mutual inhibition to implement quasi-continuous attractor configurations of stable states ~\cite{Machens2005}. In addition, the units in the temporary store of the model have excitatory connections leading back to themselves. In conjunction with mutual inhibition, this form of self-excitation implements a soft k-winner-take-all functionality which drives the activation of the stronger units higher while suppressing the activation of the weaker units (see Working Memory Dynamics for details).

A pattern of activation representing sensory input propagates across the network to produce a given response through a sequence of connections that belong to a certain pathway. Pathways are a helpful way to categorize the input-output connections according to the different tasks that they represent. For example, one pathway in the model represents the stimulus-response mappings associated with the ongoing task, and another pathway represents the stimulus-response mappings of the PM task. The speed and accuracy of performing the task ultimately depends on the strengths of the connections between the units that belong to its processing pathway. Modulating the strength of a given pathway can be achieved either by learning, which modifies the weights of the connections, or dynamically by adjusting the sensitivity of the units along the pathway. This directly translates into changes in the speed with which information spreads across the pathway and the strength of the information signal, which in turn leads to changes in response times and accuracy.

Pathways representing two distinct tasks can interact with each other when they intersect in common units or when they pass through a common layer with units that laterally inhibit each other. This can lead to competition between the two pathways which might disrupt processing and ultimately produce interference effects that manifest behaviorally as slowed down performance or decreased accuracy on one or both of the corresponding tasks. Measuring these interference effects under various conditions and comparing them with behavioral data gathered from subjects allows us to parametrize the model and test its predictions.

2.3. Model Architecture


Figure {fig:model}: High-level model architecture. OG = ongoing, PM = prospective memory. Arrows correspond to excitatory connections. Containers correspond to sets of units. The stimulus-response pathways (black containers and arrows) consist of four layers: an input layer, a feature perception layer, an associative layer, and an output layer. Information propagates along units in those layers in a feedforward fashion. The units in the associative and the output layer are in competition with each other via lateral inhibition. The ongoing and the PM pathways consist of units in these four layers which correspond to information or computations relevant for the appropriate task. The working memory module (purple containers) consists of three submodules: a task representation submodule, a feature attention submodule, and a context submodule. Units in the WM module can actively maintain their activation levels over time. The episodic memory module (teal container, units and arrows) encodes associations between different units via one-shot learning. The mahogany arrows represent top-down attentional modulation from the WM units to the stimulus-response units.

The high-level architecture of the model is illustrated in Figure \ref{fig:model} and detailed examples are shown in the Simulations section (e.g. Figure \ref{fig:focallow}). The model is a variant of the PM model by Gilbert et al. ~\cite{Gilbert2013} which is itself an extension of the Stroop model of Cohen, Dunbar, and McClelland (1990) ~\cite{Cohen1990}. The model has four processing layers that are connected to each other via bottom-up feedforward excitatory connections. The units in these layers form the different input-output pathways that allow the model to accept an input and elicit an appropriate response. The working memory (WM) module contains attentional units which modulate the activations of the units in the middle two layers in a top-down fashion. Finally, there is an episodic memory (EM) layer that supports one-shot learning of the association between the target stimulus and the PM task which triggers the switch to the PM task when the target cue is presented. Detailed descriptions of these three components follow.

2.3.1. Stimulus-Response Pathways

The activation pattern in the input layer represents the raw stimulus input that is presented on each trial. The units in the feature perception layer receive signals from the input layer and represent the various features of the input stimulus that are relevant to the given tasks. These are modulated by the feature attention units in the WM module, allowing certain aspects of the input stimulus to be enhanced and others to be suppressed dynamically in accordance with the task demands and the nature of the input stimulus. The attention-modulated input then feeds into the associative layer maps specific input features to the appropriate task responses. This layer receives additional input from the task representation units in the WM module. This allows for top-down attentional control on the task level: by sending more input to a certain set of associative units, the WM module can emphasize one task pathway over the other. That way the model can actively switch between tasks. Finally, the signals from the associative layer are synthesized in the output layer which represents the final response of the model to the particular input. Note that the perception layer does not perform any computation on the input but merely separates it into features and includes attentional modulation. Therefore, despite the fact that the overall design has four layers, the only true hidden layer that performs stimulus-response computations is the associative layer. We will collectively refer to those four layers (the input layer, the perception layer, the associative layer, and the output layer) as the stimulus-response layers.

In addition to the connections described above, there are lateral inhibitory connections between units in the associative layer. This gives rise to competition between the stimulus-response pathways of the ongoing task and the PM task which can be mediated by top-down monitoring from the attentional units in WM. Similarly, there is lateral inhibition between the units in the output layer, forcing the network to converge to a single response. Each of the stimulus-response layers (except the input layer) has an additional negative bias term that keeps the activations of all units low until they receive excitatory input either from lower-level layers or the attentional units. The negative bias terms and the weights of the lateral inhibitory connections are uniform in each layer. The weights of the connections between each pair of layers are also uniform. This creates symmetry in each layer such that all units in a given layer are structurally indistinguishable. This ensures that there is no bias hard-coded in the weights that favors one task over the other and the difference between performance on the ongoing and the PM tasks should arise solely from changes in the free parameters according to the PM instruction (see Simulations for more details).

Representations of stimuli and responses are localist. That is, every possible stimulus has a single unit in the input layer the activation of which represents whether the stimulus is present on a given trial and potentially its level of noise degradation. Similarly, every possible response has a single corresponding unit in the output layer. Using discrete rather than distributed representations simplifies the model and facilitates better understanding of the dynamics that govern its behavior. The representations could in principle be extended to distributions over many units without altering the underlying principles that apply in a localist version of the model. In addition, since the units have continuous activation values between a minimum and a maximum value, each unit can be thought of as a population of binary subnodes the average activation of which is reflected in the activation level of the localist unit.

2.3.2. Working Memory Module




Figure {fig:wm}: Connectivity within the WM module. Circles indicate inhibitory connections. The units within each submodule send strong inhibitory connections to each other (lateral inhibition) and weaker inhibitory connections to units in the other submodules. Each unit has small recurrent excitation. In addition, all units have a leak and a positive bias term corresponding to supporting excitatory current that allows them to sustain their activation over time

A detailed illustration of the WM module in shown in Figure \ref{fig:wm}. The purpose of the WM module is to represent a limited-capacity pool of units that can sustain an input pattern of activation over time even after the excitatory input that produced that pattern has ceased. All WM units are constrained by lateral inhibition which limits the total amount of activation in the WM module. This is consistent with resource models of WM ~\cite{Ma2014} which propose a limited supply of representation activation that is shared among the stored items, rather than a fixed number of independent memory slots. Unlike the stimulus-response layers, the bias of all WM units is positive, reflecting a constant inflow of excitatory current that is required to maintain an activation pattern in WM. This is is consistent with the persistent activity measured in PFC during short-term memory tasks ~\cite{Fuster1999, DEsposito2003, Machens2005}. In this way, we implemented working memory dynamics that rely on mutual inhibition rather than self-excitation. This design was inspired by the mutual-inhibition model of two-interval discrimination put forward by Machens et al. (2005) ~\cite{Machens2005} and allowed us to build a simple model of short-term memory in the PFC such that the WM units can support graded persistent activity. Furthermore, each WM unit has a very weak self-excitatory projection back to itself. The role of self-excitation in the WM module is to gradually increase the activation of the stronger units and decrease the activation of the weaker units.. This implements a soft k-winner-take-all functionality such that over longer periods of time during which WM is not updated, the WM units converge to a discrete set of states where some units are fully active and others are not active at all, subject to the overall constraints on the WM activation pool (see Working Memory Dynamics for details).

The WM module consists of three submodules. The feature attention units project to the feature perception layer and achieve the top-down monitoring of stimuli in the environment for the appropriate target cues. The task representation units project to the associative layer and provide attentional control over the ongoing and PM pathways. The context units serve as internally maintained context representations that can enhance different sets of EM traces, thus making them accessible for retrieval upon cue presentation. The strength of the mutual inhibitory connections within each submodule is stronger than the strength of the inhibitory connections between submodules. This results in greater competition among the units within a given submodule, such that a significant change in the activation of a unit will have a greater effect on units in its submodule and a smaller effect on units in the other submodules. Competition among the WM units is essential to the predictions of our model and allows it to account for many of the behavioral effects in the PM literature (see Simulations). Description of the three WM submodules follow. 

2.3.2.1. Feature Attention Units

In the simplest version of the model, there are two feature attention units. The ongoing feature attention unit projects to the feature perception units relevant for performing the ongoing task. When this attentional unit is fully active, the system is highly responsive to stimuli relevant to the ongoing task and hence facilitates faster and accurate responding in that task. Conversely, decreased activation in the ongoing feature attention unit makes the system less responsive to stimuli relevant to the ongoing task, thus attenuating ongoing task performance. Similarly, the PM feature attention unit projects to the feature perception units relevant for the PM task and thus facilitates processing of stimulus features in the PM pathway. In the current version of the model, the PM feature attention unit corresponds to the target monitoring unit in Gilbert et al.'s (2013) model ~\cite{Gilbert2013}: it monitors the environment for the target stimulus which triggers switching to the PM task on PM trials. Note that the PM feature attention unit plays a dual-role in this case: it facilitates performance on the PM task by modulating the stimulus features in the PM pathway and it also facilitates switching to the PM task by monitoring for the features of the target stimulus. While in principle these two roles could be dissociated (for example, if the target stimulus only triggers the switch but has nothing in common with the stimuli relevant for performing the actual PM task), in this article we only consider experiments with a trivial PM task that requires no computation on the input stimuli and hence a single PM feature attention unit suffices.

Lateral inhibition between the two feature attention units reflects the inherent limitations in attending to two distinct feature dimensions simultaneously: increased attention to the ongoing feature dimension decreases attention to the PM feature dimension, and vice versa. Furthermore, the biases and the weights of the projections between the feature attention units are uniform. The weights of the connections from the feature attention units to the feature perception units are also uniform. This symmetry makes the two units structurally comparable such that no feature dimension has built-in priority over the other and the model can dynamically switch between them. It also ensures that the role of attentional control in the model is strictly modulatory.

2.3.2.2. Task Representation Units

The task representation units play an equivalent role to the two task demand units in the Stroop model ~\cite{Cohen1990}. They modulate activation of the corresponding pathways by providing excitatory input to the relevant associative units. Similarly to the feature attention units on the feature perception level, increased activation in the ongoing task representation unit puts the resting activation of the ongoing task associative units in middle of their dynamic range, making them more responsive and thus leading to better performance on the ongoing task. Similarly, the PM task representation unit facilitates faster buildup of activation along the PM pathway, thus leading to faster and more accurate PM responding. Lateral inhibition between the two task representation units creates competition between the two tasks and imposes a limit on the total amount of activation shared between the two units. The dominating task is established in a dynamic equilibrium which is subject to change when external input is injected into the system. In particular, when the target stimulus is presented at the input layer and activates the PM-relevant features, the EM association which encodes the PM intention sends excitatory input that will disrupt the balance in favor of the PM task representation unit. Depending on the baseline activation of the PM task representation unit and strength of the EM input (see Episodic Memory Module below), the PM task representation unit might override the ongoing task representation unit, thus leading to a task switch and a correct PM response to the target stimulus. Finally, similarly to the feature attention units, the task representation units are symmetrical in terms of the weights of their incoming and outgoing connections.

2.3.2.3. Context Units

The context units were inspired by the internally-maintained context representations described in the temporal context model (TCM) of Howard and Kahana (2002) ~\cite{Howard2002a}. In TCM, a slowly changing internal context is associated with each new EM trace. During EM retrieval, the context that is currently active serves as a "spotlight" that illuminates traces in memory stored nearby in time ~\cite{Polyn2009}, making items studied in that context more likely to be recalled. In our model, we adopted a simplified version of this idea with only two context units: a PM context unit and a No-PM context unit which compete with each other in a fashion similar to the task representation units and the feature attention units. The PM context unit has an excitatory projection to the EM association between the target cue and the PM task unit, thus modulating its strength. In this way, the activation of the PM context unit gates the episodic recall of the PM intention (see Episodic Memory Module for more details). The role of the PM context unit is thus to represent whether the PM task is currently relevant. Note that this role is different from the role of the PM task representation unit which controls whether the PM task is currently being performed. To illustrate the role of the PM context unit, imagine a scenario in which the subject is instructed to temporarily suspend the PM intention and only perform the ongoing task, even in the presence of PM targets. In our model, suspending PM intention is equivalent to lowering the activation of the PM context unit which decreases the sensitivity of the EM association between the target and the PM task unit, thus disabling task switching on targets. Once the PM task becomes relevant again, activation of the PM context unit is increased, which in turn reinstantiates the target-PM task association and reenables task switching.

While this implementation of internally-maintained context may seem trivial, it gives the model the powerful ability to flexibly turn on or turn off the relevance of the PM task which allows us to capture phenomena associated with temporary suspension of PM intention as well as aftereffects of intention once the PM intention has been permanently suspended (see Simulation of Experiment 5). This design is consistent with theoretical and empirical findings that task shifts result in significant changes in temporal context. In addition, this simple implementation of temporal context can be expanded to include other elements of TCM such as contextual drift, allowing the model to capture PM effects that are beyond the interest of the present article (see General Discussion).

2.3.3. Episodic Memory Module

As discussed in the introduction, EM is thought to be responsible for associating the target stimulus with the PM task instruction, and retrieving the latter when the former appears. According to the complementary learning systems theory ~\cite{McClelland1995}, the system that supports this form of rapid content-addressable associative memory in the human brain is the hippocampus. Our choice of design for the EM store in the model was therefore inspired by the temporal context model (TCM) of Howard and Kahana (2002) ~\cite{Howard2002a}. We include an EM module that contains a (theoretically unlimited) supply of nodes which can be used to form novel multi-way associations between other units in the network. Each unit encodes a single EM trace by establishing connections with all units that are active at the time of encoding.

A novel PM instruction is thus encoded as a three-way association between the target unit in the feature perception layer, the PM context unit, and the PM task representation unit, mediated by a previously unused EM node. We assume that during the PM instruction, the subject is explicitly encoding an association between the target and the PM task, whereas the context is implicitly stamped in with every EM association in accordance with TCM. This captures the idea that the presence of a particular stimulus can “bring to mind” a certain intention or response as pattern of activation in WM via the associations in EM. For simplicity, we assume that the connections are one-directional, such that the context unit and the target feature unit send excitatory input to the EM node which in turn sends excitatory input to the PM task unit. In this way, when the PM target is presented and the PM context is sufficiently active, the model switches to performing the PM task. Further, we assume uniform weights for all EM associations. Thus successfully switching to the PM task on a PM trial depends on the activation of the PM context unit and the activation of the target feature unit, which in turn depends on the activation of the PM feature attention unit (i.e. the level of target monitoring). A more elaborate model of the hippocampus with flexible connection weights and richer context representations would perhaps be necessary to account for a wider range of PM phenomena (see General Discussion). However the present design is sufficient for the purposes of illustrating the main PM effects that result from the interaction between EM and WM.

2.4. Operation of the Model

Every simulation begins with a PM instruction followed by a sequence of trials in which on each trial a pattern of activation representing the trial stimulus is presented at the input layer and a response is produced at the output layer. The mechanisms that govern the time course of processing in the model are described below. It is worth noting that all of the model parameters described here were kept constant across all simulations, except where explicitly noted.

2.4.1. Stimulus-Response Dynamics

We adopted the mechanism for simulating the time course of processing from the Stroop model of Cohen, Dunbar, and McClelland (1990) ~\cite{Cohen1990}. A trial begins by activating a set of units at the input layer of the model corresponding to the stimulus presented at the given trial. The activation levels of these units are set to 1 and their value is maintained throughout the entire trial. The activation of the remaining input units is 0. Activation then proceeds to spread throughout rest of the network in several cycles according to the cascade model introduced by McClelland (1979) ~\cite{McClelland1979}. Each cycle corresponds to a discrete time step during which the activation of each unit is updated according to the weighted sum of the activation levels of the units that project to it. In particular, instantaneous net input to unit $j$ at time $t$ is equal to

TODO noise noise noise!! gaussian \sigma_{\text{S-R}} = 0.1

where $a_i(t-1)$ is the activation level of unit $i$ during the previous cycle and $w_{ij}$ is the weight of the connection from unit $i$ to unit $j$. In addition, each unit has a negative bias term that can keep the net input negative unless the unit is receiving excitatory signals from adjacent units. The activation for unit $j$ is then calculated as a logistic function of its averaged net input:

where $\overline{\text{net}}_j(t)$ is the weighted average of the net input to unit $j$ over the course of several cycles:


The parameter $\tau$ is a rate constant that establishes the time course of processing of the model. Small values of $\tau$ lead to a slower processing since the new net input at time $t$ has a lower weight in the equation; conversely, larger values of $\tau$ lead to faster processing. In all simulations, we set $\tau = 0.1$. The logistic function in the activation update equation introduces nonlinearity in the system and also constrains the activation of each unit between 0 and 1.

The nonlinear update equation has significant benefits over linear update equations which impose significant computational limitations on networks even if they have multiple layers ~\cite{Rumelhart1986}. In fact, the nonlinearity of the activations allows the attentional units to modulate the responsiveness of the different pathways by altering the resting state of their corresponding units. Since the activation is governed by the logistic function, when the net input to a unit has a large negative value, its activation remains close to 0 and changes in the net input have a small effect on it. Similarly, for large positive net inputs, the activation level of the unit is close to 1 and remains relatively stable for changes in the net input. In contrast, when the net input to a unit is around 0, the slope of the activation function is approximately linear. Thus the unit is in its most responsive range where changes in its net input have a significant effect on its activation. Therefore when a unit whose net input is negatively biased receives excitatory input from the attentional units in WM, it moves closer to its maximally responsive dynamic range and response more strongly to input from the lower layers of the network.

2.4.1.1. Response Selection

The response of the network is determined by activation in the output layer. To model the variability of the response, we adopted the response mechanism from the Stroop model ~\cite{Cohen1990} which implements a drift diffusion process in which each output unit has a corresponding accumulator that undergoes a random walk. Each evidence accumulator starts with a value of 0 and on each cycle a small amount evidence is added to its value. The amount of evidence added to accumulator $i$ is a random number from a Gaussian distribution a fixed standard deviation $\sigma_{\text{acc}}$ and mean based on the activation of output unit $i$:

The model produces a response when one of the evidence accumulators reaches a predefined threshold at which point the trial ends, the response of that accumulator is recorded as the model’s response on that trial and the number of cycles is recorded as the response time. If none of the accumulators reaches the threshold within a certain number of cycles, the trial ends and a response timeout is recorded. This is consistent with most PM paradigms in which the subject has a limited amount of time to respond on each trial. In addition, the diffusion process is consistent with response selection models of psychophysical processes based on a random walk ~\cite{Link1975} or a diffusion process ~\cite{Ratcliff1978}. In all of our simulations, we used a response threshold value of 1 and values of $\alpha = 0.05$ and $\sigma_{\text{acc}} = 0.05$. If none of the response accumulators reaches threshold before a certain time limit, the trial ends and a default timeout response is recorded. The response timeout we used in all simulations was 500 cycles.

Once a response is made, evidence accumulators are reset to zero. The TODO. The activations of the WM units are not altered, unless the activation in the PM task representation unit is greater than a certain threshold, which implies that a task switch occurred on the least trial. In that case, the task representation units are restored to their initial activation levels determined at PM instruction (see PM Task Switching). We assume that switching to the PM task acts as a trigger for an episodic association that reconstitutes the ongoing task as the default task. The activation levels of the other WM units are not changed. Since the focus of the model is on the dynamics of switching from the ongoing task to the PM task, rather than switching back to the ongoing task after a PM trial, the precise mechanism behind the process of restoring the ongoing task is beyond the interest of this article. We assume that it occurs through an implicit episodic association between the act of responding, the PM task unit, and the ongoing task unit. See the General Discussion section for a discussion on how this reverse switch can be incorporated as part of the EM store in the model.

2.4.2. Episodic Memory Dynamics

We used the same mechanics and update equations for the EM store as for the stimulus-response pathways, with the only difference that we set the EM activations to 0 if they are below a certain threshold, as follows:

For every EM unit $j$. This thresholding is necessary to prevent the EM units from introducing a constant background bias to the WM units, and in particular the PM task unit. Since the sigmoid activation function is always positive, even large negative inputs to EM result in some small excitatory current to the PM task unit. Due to their dynamics, the WM units are highly sensitive to any type of input (see Working Memory Dynamics) and this small current can significantly alter the long-term activations in WM. We recognize that this approach is an artificial shortcut that is not part of the PDP framework and we suggest ways to overcome it in the General Discussion section.

2.4.3. Working Memory Dynamics

We implemented the WM units as leaky competing accumulators (LCA’s) ~\cite{Usher2001}. The activation of WM unit $j$ is thus governed by the equation:

where $\rho$ represents the feed-forward input to WM unit $j$, $\lambda$ is the decay rate, $\gamma$ is the scaling factor for the recurrent excitation, $\beta_{ij}$ is the weight of the inhibitory influence from WM unit $i$ to WM unit $j$, $w_{ij}$ is the weight of the input from non-WM unit $i$ to WM unit $j$, and $f_i$ is the activation-to-output transfer function. In the model, we used a linear transfer function ($f_i = x_i$), which allows us to simplify the equation to

Note that this equation is different from the one governing the activation of the stimulus-response units (Eq \ref{eq:actSR}). Since the role of the WM units is strictly modulatory rather than computational, there is no need to introduce nonlinear dynamics in their activations. At the same time, using linear transfer functions allowed us to configure the LCA's as line attractors that sustain a continuous range of stable states. This decision is plausible considering that stimulus-response processing and top-down attentional control occur in different brain regions through different physiological dynamics. More importantly, this difference is not critical to the predictions of our model and can be overcome using a more sophisticated model of WM (see General Discussion). In this article, we strove for the simplest model of WM that supports graded persistent activation. The values of the constants used in the model are $dt = 0.05$, $\rho = 4$, $\lambda = 2$, $\gamma = 0.0004$ $\beta_{ij} = 2$ if $i$ and $j$ belong to the same WM submodule, and $\beta_{ij} = 1$ otherwise. The parametrization of the WM units is explained below.

In order to make the WM units compatible with the rest of the network, we thresholded their activation levels between 0 and 1 so that they fall within the same range as the output of the logistic function (Eq \ref{eq:actSR}). To simplify notation, we can use the same weight matrix $\mathbf{w}$ for the weights of the stimulus-response connections and the mutual inhibitory influence weights of the WM LCA's by setting $w_{ij} = -\beta_{ij} \,\,\, \forall i,j\in WM$. Due to the linear transfer function, we can account for the leak rate and the self-excitation of the LCA's using the same notation by setting $w_{jj} = \gamma - \lambda \,\,\,\forall j\in WM$. Finally, we can use the same notation for the bias term in the stimulus-respone layers and the excitatory current to the LCA's by setting $\text{bias}_{j} = \rho \,\,\,\forall j\in WM$. This allows us to rewrite the activation equation for the LCA units into:

TODO noise noise noise!! Gaussian; \sigma_{\text{S-R}} = 0.01

Where $\text{net}_j$ is calculated using the same equation as the stimulus-response units (Eq \ref{eq:netSR}).

2.4.2.1. Continuous Attractors in WM

As discussed in the previous section, the purpose of the WM module is to serve as a limited-capacity activation-based temporary memory store that can support the top-down monitoring of the stimulus-response pathways. In order to support graded levels of monitoring, we designed the WM units as LCA's with continuous attractor dynamics. Line attractor networks have been used to model short-term memory store of eye position ~\cite{Seung1996}, spatial working memory ~\cite{Wang2001}, head direction ~\cite{Zhang1996, Taube1998}, somatosensory input ~\cite{Machens2005}, and other forms of graded persistent activity ~\cite{Brody2003} as a substrate for short-term memory. Our specific design was inspired by the model of of two-interval discrimination presented by Machens et al. (2005) ~\cite{Machens2005}. Their design consists of two mutually inhibitory neurons that support a quasi-continuous range of attractor states. During the loading phase, an initial input current puts the network in a given state of activation. The input-output transfer function and the excitatory current are chosen such that once the input current ceases, the network creates a line attractor of stable points and initiates maintenance mode whereby it sustains the level of activation that was determined by the input current. Finally, to make the comparison/decision, the network is put in a state with only two stable points which forces the activation to converge to a binary decision. 

We designed the WM submodules so that they can support the loading and the maintenance phase. The parameters were chosen such that each pair of LCA's has an infinite number of stable states along the continuum between 0 and 1. For a simple two-unit LCA's linear transfer functions, this is achieved by setting the mutual inhibition parameter equal to the leak in Eq \ref{eq:actWMLCA}. To demonstrate this, consider a pair of LCA's which have activations $x_1$ and  $x_2$ governed by the equations:

If we set $k = \beta$, this dynamic system has an infinite number stable points. In particular, the derivative of both activations is zero for all activation levels that satisfy the equation:

Thus, starting from some initial activation, the two attractors will converge to the closest stable point that satisfies this equation. Notice that since the sum of the two activations is the same for all equilibrium points, the two LCA's are in direct competition with each other -- an increase in the activation in one unit directly leads to a decrease in the activation of the other. Furthermore, in the absence of external input, the difference between the two activation levels remains constant over time:


This dynamic is implemented within in each WM submodule. The values of $dt = 0.05$ and $\beta = k = 2$ were chosen so that the LCA's operate on a time scale that is comparable to that of the stimulus-response layers (Eq \ref{eq:netavgSR}). The value of the excitatory current $\rho$ was then chosen so that the sum of the two activations (Eq \ref{eq:sumLCA}) is always 1. In the case of two units, this is achieved for $\rho = 2$ (although a stronger current is necessary to support multiple LCA's -- see below). To illustrate the behavior of a WM submodule, consider the two task representation units shown in Figure \ref{fig:wm} in isolation, as well as their mock activation profiles in Figure \ref{fig:taskswitch-a}. Starting off with activation levels of 1 and 0.6 for the ongoing task unit and the PM task unit, respectively, after several cycles, the network settles at activation levels of 0.7 and 0.3. Notice that the difference between the resting activations is the same as that between the initial activations. Hence this initial difference determines in which state the two activations will settle along the line attractor. The two LCA's then sustain these resting activation levels over the course of several trials. Since the ongoing task unit has greater activation, the model performs the ongoing task for a few trials, until it encounters a PM target. On a PM trial, the PM target activates the EM association which sends strong a excitatory signal to the PM task unit. This unbalances the system and sends the state along the attractor line towards a higher PM task activation and a lower ongoing task activation. A task switch thus occurs and the model performs the PM task on the given trial. After the PM trial, the task unit activations are reset to their initial levels (see Response Selection) and the network is allowed to settle in preparation for the next trial.

The same mechanism is implemented in for the context submodule and the feature attention submodule. In order to reflect the overall limitation on WM capacity, we also included inhibitory connections between the units from different submodules. In this way, all units in WM are competing for the same pool of activation resources which can be regulated through the magnitude of the supporting excitatory current $\rho$. In order to preserve the direct competition within each submodule, we made the cross-module inhibitory connections weaker than the mutual inhibitory connections within the submodules. In particular, we chose a value of 1 (half of the mutual inhibitory weight $\beta$). That is, every unit in WM has an inhibitory projection of weight 1 to every other unit in WM, unless the two units belong to the same WM submodule (feature attention, task representation, or context) in which case the weight of the inhibitory projection is 2. To illustrate the effect of this, consider what happens when we include the two feature attention units in addition to the task representation units (Figure \ref{fig:taskswitch-b}). After initialization, the four units still settle and behave as two independent LCA's. When the task switch occurs, the rapid change in the PM task unit causes a small perturbation of the feature attention units. However, the shock is mostly absorbed by the ongoing task unit because the PM task unit sends a stronger inhibitory signal to that unit than it does to the feature attention units, so the feature attention units quickly return to the resting state along their attractor continuum (notice that this also introduces a certain level of robustness in the WM units which is often thought to be incompatible with perfect line attractors ~\cite{Brody2003}). The overall limitation on WM becomes apparent when a significant perturbation occurs as a result of strong external signal that disrupts several units at once (Figure \ref{fig:taskswitch-c}), or when an extra load is introduced via and additional unit in WM (Figure \ref{fig:taskswitch-d}). In that case, the activation levels of all units are decreased, in accordance with the view that there is a fixed limited supply of activation that is shared among all units in WM  ~\cite{Ma2014}. The value of the supporting current $\rho = 4$ was thus chosen in order to support the maximum activation level of 1 for a single unit in each of the three WM submodule while all other units have activation levels of zero.


Figure {fig:taskswitch}: Sample activation profiles that illustrate the behavior of the continuous attractors in subsets of the WM units.

Figure {fig:wm-decay}: K-winner-take-all dynamics in the WM module. Due to the small self-excitatory current, the stronger unit in each submodule suppresses the weaker unit such that over long periods of time, the LCA's act like binary attarctors.

Finally, each WM unit has a small recurrent excitatory current $\gamma$ (Eq \ref{eq:actWMLCA}). We used this current to implement a form of decay of WM such that, over longer periods of time without any updates to WM, stronger units tend to suppress weaker units. In the current system, this results in a k-winner-take-all dynamic that elevates the strongest unit from each submodule towards an activation of 1 while pushing the other units to an activation of 0. An illustration of this behavior is shown in Figure \ref{fig:wm-decay}. The value of $\gamma = 0.0004$ was chosen according to the empirically measured decay in prospective remembering (see Experiments 1 and 2). Thus each WM submodule implements a quasi-continuous attractor network which behaves like a line attractor over short time scales (i.e. the LCA's can sustain a graded level of activation) and like a binary attractor over long time scales (i.e. the LCA's converge to a state where one unit has maximum activation and the others have minimum activation). K-winner-take-all functionality has previously been used in models of WM ~\cite{OReilly2006} and is consistent with the notion that WM fails at retaining the same activation pattern over longer time intervals ~\cite{Ma2014}. In addition, the quasi-linear attractors add a certain level of plausibility to the WM module since perfect line attractors are not found in real biological systems ~\cite{Seung1998}.

2.4.4. PM Instruction

Each sequence of trials is preceded by a PM instruction that specifies the PM condition and the target stimuli. During the PM instruction, two things occur.

First, the model encodes the PM target in the EM store by creating a new three-way association between the target feature units in the feature perception layer, the PM context unit, and the PM task representation unit, with a single EM unit receiving an input connection from the former two and sending an excitatory projection to the latter (see Episodic Memory Module). This implements the encoding phase of the spontaneous retrieval process in the multiprocess theory of PM ~\cite{McDaniel2000, EM2005} according to which, during planning, the subject forms an association between the target cue and the intended action. The values of the weights used in all simulations can be found in the Parametrization section of Simulation 1.

Second, the network is put in preparatory mode by manually setting the initial activations of the WM units to values that put them in a state which allows the model to perform the task successfully according to the PM instructions. We maintain these activations manually for 40 cycles after which we allow the network to settle. We decide that the network has settled using the average change in activation across all nodes over the last 60 cycles, as follows:

Over all nodes $j$, where $t$ is the current cycle. We also check that the standard deviation of the above is less than 0.0001 and that at least 60 cycles have passed since the start of the simulation. Once all of these conditions are met, we declare that the network has settled (see Working Memory Dynamics) and begin presenting the sequence of stimuli corresponding to the trials, starting with the first trial. When the WM units settle, the different levels of activation of the different units in WM reflect the different amounts of top-down monitoring that the model exerts over the different stimulus-response pathways in the network. This captures the monitoring process in the multiprocess theory ~\cite{McDaniel2000, EM2005} which suggests that in some conditions, preparatory attentional processes are necessary for successful PM retrieval. Varying the initial WM activations associated with different PM conditions allowed us to explore the tradeoffs that occur as a result of the limitations on WM and how they give rise to the behavioral PM effects described in the literature. For example, high emphasis on the PM task would correspond to more activation in the PM task representation unit and the PM feature attention unit, and conversely, low emphasis on the PM task would be encoded as less activation in these units. This is will lead to correspondingly more or less top-down monitoring of the PM pathway compared to the ongoing pathway, resulting in different RT’s and PM hit rates for both tasks. Since the weights of the connections are uniform across the different pathways, the initial amount of activation in the WM units for the ongoing task is higher than for the PM task in all simulations. Since no task has a higher priority built into the architecture of the model, this is the only way the model can perform the ongoing task as the default task, and the roles of the two tasks could be swapped by simply flipping the initial WM activations of their corresponding units in WM.

2.4.5. Task Switching

At the end of each trial, we check whether the model has switched to performing the PM task by comparing the PM task unit activation level with its resting activation level. We select the activation levels when the network settles on the first trial as the resting activation levels and use these values for all subsequent trials. We determine that a task switch has occurred if the following condition is met:

Where $t$ denotes the cycle at the end of a trial. In this case, we manually reset the activation of the PM task unit and the OG task unit to their activations as they were when the network settled on the last trial.

When the model subject detects a PM target, they either fully or partially activate their PM task unit. If the PM task unit is activated above the threshold in Eq {XX} above, we consider that the model subject is conscious of the task switch, regardless of whether they performed a PM response or not, and that afterwards they consciously modulate their task units according to the instructino in preparation for performing the OG task on the subsequent trial. Conversely, if the PM task unit partially activated but is below the threshold in Eq {XX}, we consider that the subject has not become aware of the task switch and thus may maintain a slightly elevated PM task unit for subsequent trials.

While less than perfect, this simple mechanism allows us to model the main effects of PM without introducing unnecessary complexity whose effects would be difficult to account for with the empirical data. The value of the threshold in Eq. {XX} was chosen based on the fact that, due to the k-winner-take-all dynamics in WM (see Working Memory Dynamics), the PM task unit activation will slowly decay after the first trial. As a consequence, the PM task unit activation will almost always be below this resting activation level as obtained at the start of the experiment. Since its activation at the start of the experiment is the result of the initial PM task activation that was assigned during the preparatory PM instruction phase, we assume that a similar mechanism exists for restoring the task activation whose exact details are beyond the scope of the present study. More importantly, the precise value of the threshold in Eq. {XX} bears no significant influence on the main effects measured in our simulations. In future iterations of the model, we envision that the process of switching back to the ongoing task would be part of the PDP network (see General Discussion).

2.4.6. Sources of Variance

In order to account for the variability in the data, we introduced sevaral sources of noise in the model. These can broadly be divided into within-subject and cross-subjet sources of noise, the former accounting for differences in RT's and accuracies on different trials during a single run with the same parameters (i.e. a single subject), and the latter accounting for systematic differences between individual subjects or groups of subjects.

Within-subject variance is the result of processing noise in the network and noise in the evidence accumulation units. As discussed in the Stimulus-Response Dynamics section, a small Gaussian noise with $\sigma_{\text{S-R}} = 0.1$ is added to the net input of each unit at every cycle. Similarly, as discussed in the Working Memory Dynamics section, Gaussian noise with $\sigma_{\text{WM}} = 0.01$ is added to the net inputs of all WM units. The value of $\sigma_{\text{S-R}}$ is standard for this type of network ~\cite{Cohen1990}. The value of $\sigma_{\text{WM}}$ is lower because of the greater sensitivity of the WM units to input perturbations. The Gaussian noise used for the drift diffusion process of action selection in the evidence accumulators has $\sigma_{\text{acc}} = 0.05$. These values were consistent across all subjects in all simulations.

Cross-subject variance is the result of noise added to the initial WM activations and WM bias introduced prior to the start of each simulation of a given subject. Gaussian noise with $\sigma_{\text{init WM}}$ is added to the initial activations of the PM task unit and the PM feature attention unit. These represent the different levels of top-down monitoring that different subjects may exert based on the PM instruction they receive. Note that due to the WM dynamics, adding noise to the initial OG task activation and OG feature attention activation is redundant. The values of $\sigma_{\text{init WM}}$ used in the different simulations are presented in the Simulations section. Finally, Gaussian noise with $\sigma_{\text{WM bias}} = 0.1$ is added to the biases of all WM units. The WM units require a constant excitatory current to maintain their activations (see Working Memory Dynamics section) and reducing that current correspondingly reduces the total amount of activation and, due to the k-winner-take-all dynamics, the total amount of information that can be stored in WM. This results in weaker top-down monitoring for the S-R pathways which in turn hampers RT's and accuracies on both tasks. Thus reducing the WM bias for a given model subject is equivalent to reducing their WM capacity.


3. Simulations

Our first simulation was of Experiment 1 in Einstein, McDaniel et al. (2005) ~\cite{EM2005}. In Experiment 1, they examined the effects of manipulating three different variables: the focality of the target event, the emphasis on the PM task, and the presence or absence of the PM task. A target is considered to be focal if there is a significant overlap between the target features and the stimulus features relevant for performing the ongoing task. When the target is focal, performing the ongoing task simultaneously allocates attention to the features of the target stimulus and thus its processing ~\cite{Maylor1996, Maylor2002}. A real world example of a focal target is remembering to give a message to a colleague when you see them at work. According to the multiprocess theory of PM, spontaneous retrieval is more likely in this case since processing the target (your colleague) occurs as part of your routine ongoing activity (work). An example of a nonfocal target would be remembering to call and give a message to a colleague while you’re on vacation. In this scenario, the target (your colleague) is not processed as part of your ongoing activity (vacationing). In that case, according to the multiprocess theory, spontaneous retrieval is less likely and successful performance of the PM task would require top-down monitoring.

TODO figure



Sources of variance

